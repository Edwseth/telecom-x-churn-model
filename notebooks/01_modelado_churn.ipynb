{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelado de churn de Telecom X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga de datos\n",
        "Se importan librerías principales y se carga el dataset tratado.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df = pd.read_csv('data/telecom_clientes_tratado.csv')\n",
        "df['Charges.Total'].fillna(df['Charges.Total'].mean(), inplace=True)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploración y limpieza\n",
        "Revisión de valores faltantes y eliminación de columnas irrelevantes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Eliminación de identificadores y exploración inicial\n",
        "df.drop(columns=['customerID'], inplace=True, errors='ignore')\n",
        "churn_rate = df['Churn'].value_counts(normalize=True)\n",
        "print('Distribución porcentual de churn:')\n",
        "print(churn_rate)\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Distribución de clientes con y sin churn')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Codificación y preparación de variables\n",
        "Transformación de variables categóricas y separación de variables predictoras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "X = df_encoded.drop('Churn_Yes', axis=1)\n",
        "y = df_encoded['Churn_Yes']\n",
        "df_encoded.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de correlación y visualizaciones\n",
        "Relaciones entre variables y la variable objetivo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "sns.heatmap(df_encoded.corr(), cmap='coolwarm', center=0)\n",
        "plt.title('Matriz de correlación')\n",
        "plt.show()\n",
        "cor_target = df_encoded.corr()['Churn_Yes'].sort_values(ascending=False)\n",
        "print(cor_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sns.boxplot(data=df, x='Churn', y='tenure')\n",
        "plt.title('Tenure vs Churn')\n",
        "plt.show()\n",
        "sns.boxplot(data=df, x='Churn', y='Charges.Total')\n",
        "plt.title('Gasto Total vs Churn')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## División de datos y escalado\n",
        "Separación en conjuntos de entrenamiento y prueba y escalado de variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Balanceo de clases con SMOTE\n",
        "Se aplica SMOTE para equilibrar la proporción de clases antes del entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "scaler_smote = StandardScaler()\n",
        "X_train_smote_scaled = scaler_smote.fit_transform(X_train_smote)\n",
        "X_test_smote_scaled = scaler_smote.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento de modelos (Logistic Regression, Random Forest, XGBoost)\n",
        "Entrenamiento de diferentes algoritmos para predecir la cancelación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Regresión Logística\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr.fit(X_train_smote_scaled, y_train_smote)\n",
        "\n",
        "# Random Forest con búsqueda de hiperparámetros\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf_random = RandomizedSearchCV(rf, param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train_smote, y_train_smote)\n",
        "rf_best = rf_random.best_estimator_\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "xgb.fit(X_train_smote, y_train_smote)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluación de modelos y ajuste de umbrales\n",
        "Medición de desempeño y ajuste del umbral para Regresión Logística.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Predicciones y evaluación con umbral estándar\n",
        "print('📊 Logistic Regression (umbral=0.50):')\n",
        "print(classification_report(y_test, lr.predict(X_test_smote_scaled)))\n",
        "print(confusion_matrix(y_test, lr.predict(X_test_smote_scaled)))\n",
        "\n",
        "print('📊 Random Forest:')\n",
        "print(classification_report(y_test, rf_best.predict(X_test)))\n",
        "print(confusion_matrix(y_test, rf_best.predict(X_test)))\n",
        "\n",
        "print('📊 XGBoost:')\n",
        "print(classification_report(y_test, xgb.predict(X_test)))\n",
        "print(confusion_matrix(y_test, xgb.predict(X_test)))\n",
        "\n",
        "# Ajuste de umbral para Regresión Logística\n",
        "umbral_optimo = 0.35\n",
        "y_probs = lr.predict_proba(X_test_smote_scaled)[:, 1]\n",
        "y_pred_umbral = (y_probs >= umbral_optimo).astype(int)\n",
        "print(f'\nEvaluación Logistic Regression con umbral {umbral_optimo}:')\n",
        "print(classification_report(y_test, y_pred_umbral))\n",
        "print(confusion_matrix(y_test, y_pred_umbral))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curva ROC - Regresión Logística')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importancia de variables\n",
        "Identificación de las variables más relevantes en los modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lr_importance = pd.Series(lr.coef_[0], index=X.columns).sort_values(ascending=False)\n",
        "print('Importancia (coeficientes) - Logistic Regression:')\n",
        "print(lr_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rf_importance = pd.Series(rf_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print('Importancia - Random Forest:')\n",
        "print(rf_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparación y resultados\n",
        "Resumen de métricas para cada modelo entrenado.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def metricas(y_true, pred):\n",
        "    report = classification_report(y_true, pred, output_dict=True)\n",
        "    return [report['accuracy'], report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score']]\n",
        "\n",
        "resultados = pd.DataFrame(\n",
        "    index=['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
        "    columns=['Accuracy', 'Precision', 'Recall', 'F1']\n",
        ")\n",
        "resultados.loc['Logistic Regression'] = metricas(y_test, y_pred_umbral)\n",
        "resultados.loc['Random Forest'] = metricas(y_test, rf_best.predict(X_test))\n",
        "resultados.loc['XGBoost'] = metricas(y_test, xgb.predict(X_test))\n",
        "print(resultados)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión estratégica\n- **Variables influyentes:** permanencia del cliente (tenure), contratos mes a mes, falta de soporte técnico y cargos totales elevados.\n- **Mejor modelo:** XGBoost presentó el mayor desempeño global frente a Random Forest y Regresión Logística.\n- **Perfil de cliente propenso a cancelar:** usuarios de corta permanencia con servicios mensuales y pagos mediante *electronic check*.\n- **Acciones de retención:** ofrecer descuentos por contratos anuales, reforzar soporte técnico y campañas proactivas para clientes con altos cargos mensuales.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}